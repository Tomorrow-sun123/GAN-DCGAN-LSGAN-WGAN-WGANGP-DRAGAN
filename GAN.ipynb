{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics setting\n",
    "g_loss_metrics = tf.metrics.Mean(name='g_loss')\n",
    "d_loss_metrics = tf.metrics.Mean(name = 'd_loss')\n",
    "total_loss_metrics = tf.metrics.Mean(name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper_parameters\n",
    "ITERATION = 1000\n",
    "Z_DIM = 100\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 60000\n",
    "IMAGE_SIZE = 28*28\n",
    "D_LR = 0.0004\n",
    "G_LR = 0.0004\n",
    "IMAGE_SHAPE = (28,28,1)\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_z = tf.random.normal([36,Z_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_z(batch_size,z_dim):\n",
    "    return tf.random.uniform([batch_size,z_dim],minval=-1,maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define discriminator\n",
    "def make_discriminator(input_shape):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Input(IMAGE_SHAPE),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256,activation = None,input_shape = input_shape),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dense(256,activation = None),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dense(1,activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define generator\n",
    "def make_generator(input_shape):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(256,activation='relu',input_shape=input_shape),\n",
    "        layers.Dense(256,activation='relu'),\n",
    "        layers.Dense(784,activation = 'tanh'),\n",
    "        layers.Reshape(IMAGE_SHAPE)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "def get_loss_fn():\n",
    "    def d_loss_fn(real_logits,fake_logits):\n",
    "        return -tf.reduce_mean(tf.math.log(real_logits + 1e-10) + tf.math.log(1.- fake_logits + 1e-10))\n",
    "    def g_loss_fn(fake_logits):\n",
    "        return -tf.reduce_mean(tf.math.log(fake_logits + 1e-10))\n",
    "    \n",
    "    return d_loss_fn,g_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load & preprocessing\n",
    "(train_x,_),(_,_) = tf.keras.datasets.mnist.load_data()\n",
    "train_x = (train_x - 127.5)/127.5\n",
    "train_x = train_x.astype(\"float32\")\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_x)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE,drop_remainder = True)\n",
    "    .repeat()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator & discriminator\n",
    "G = make_generator((Z_DIM,))#Z_DIM = 100\n",
    "D = make_discriminator((IMAGE_SIZE,))#IMAGE_SIZE = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "g_optim = tf.keras.optimizers.Adam(G_LR)\n",
    "d_optim = tf.keras.optimizers.Adam(D_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "d_loss_fn,g_loss_fn = get_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    #z = get_random_z(Z_DIM,BATCH_SIZE)#Z_DIM = 100,BATCH_SIZE = 512\n",
    "    z = get_random_z(BATCH_SIZE,Z_DIM)#Z_DIM = 100,BATCH_SIZE = 512\n",
    "    with tf.GradientTape() as d_tape,tf.GradientTape() as g_tape:\n",
    "        fake_images = G(z,training=True)\n",
    "        \n",
    "        fake_logits = D(fake_images,training = True)\n",
    "        real_logits = D(real_images,training = True)\n",
    "        \n",
    "        d_loss = d_loss_fn(real_logits,fake_logits)\n",
    "        g_loss = g_loss_fn(fake_logits)\n",
    "        \n",
    "    d_gradients = d_tape.gradient(d_loss,D.trainable_variables)\n",
    "    g_gradients = g_tape.gradient(g_loss,G.trainable_variables)\n",
    "    \n",
    "    d_optim.apply_gradients(zip(d_gradients,D.trainable_variables))\n",
    "    g_optim.apply_gradients(zip(g_gradients,G.trainable_variables))\n",
    "    \n",
    "    return g_loss,d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def train(ds,log_freq=20):\n",
    "    ds = iter(ds)\n",
    "    for step in range(ITERATION):\n",
    "        images = next(ds)\n",
    "        images = images[...,tf.newaxis]\n",
    "        g_loss,d_loss = train_step(images)\n",
    "        \n",
    "        g_loss_metrics(g_loss)\n",
    "        d_loss_metrics(d_loss)\n",
    "        total_loss_metrics(g_loss +d_loss)\n",
    "        \n",
    "        if step % log_freq == 0:\n",
    "            template = '[{}/{}] D_loss={:.5f} G_loss={:.5f} Total_loss={:.5f}'\n",
    "            print(template.format(step,ITERATION,d_loss_metrics.result(),\n",
    "                                 g_loss_metrics.result(),\n",
    "                                 total_loss_metrics.result()))\n",
    "            g_loss_metrics.reset_states()\n",
    "            d_loss_metrics.reset_states()\n",
    "            total_loss_metrics.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
